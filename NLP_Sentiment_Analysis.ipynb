{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for NLP interpretation\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "\n",
    "# import own scripts\n",
    "import sys\n",
    "sys.path.insert(0, '/src/')\n",
    "\n",
    "%autoreload 2\n",
    "from utils import format_raw_documents, myNLP, merge_2_string_lists, get_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: \n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores: \")\n",
    "!sysctl -n hw.ncpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook functions\n",
    "def parallelize(function, tasks, workers=4):\n",
    "    '''\n",
    "    Performs a task as defined by `function` in parallel and returns the result.\n",
    "    '''\n",
    "    if __name__ == '__main__':\n",
    "        with Pool(processes=workers) as p:\n",
    "            results = list(tqdm_notebook(p.imap(function, tasks), total=len(tasks)))   \n",
    "        return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142616</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>oil prices up in asian trad</td>\n",
       "      <td>Singapore: Oil prices edged higher in quiet As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142617</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>cost of quaid e azam solar power project cut b...</td>\n",
       "      <td>LAHORE: Putting  stout defence of the solar po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142618</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>china pakistan set to sign cpec zone d</td>\n",
       "      <td>KARACHI: Pakistan is set to sign a 40-year-lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142619</th>\n",
       "      <td>9/9/2016</td>\n",
       "      <td>Global airport traffic 64 percent 2015 fastest...</td>\n",
       "      <td>strong&gt;MONTREAL: Global airport traffic grew a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142620</th>\n",
       "      <td>9/9/2016</td>\n",
       "      <td>Taxpayers directory to be launched today</td>\n",
       "      <td>strong&gt;ISLAMABAD: Finance Minister Ishaq Dar w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                              Title  \\\n",
       "142616  9/9/2015                        oil prices up in asian trad   \n",
       "142617  9/9/2015  cost of quaid e azam solar power project cut b...   \n",
       "142618  9/9/2015             china pakistan set to sign cpec zone d   \n",
       "142619  9/9/2016  Global airport traffic 64 percent 2015 fastest...   \n",
       "142620  9/9/2016           Taxpayers directory to be launched today   \n",
       "\n",
       "                                                  Content  \n",
       "142616  Singapore: Oil prices edged higher in quiet As...  \n",
       "142617  LAHORE: Putting  stout defence of the solar po...  \n",
       "142618  KARACHI: Pakistan is set to sign a 40-year-lea...  \n",
       "142619  strong>MONTREAL: Global airport traffic grew a...  \n",
       "142620  strong>ISLAMABAD: Finance Minister Ishaq Dar w...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load news articles\n",
    "a1 = pd.read_csv('data/Articles.csv', encoding = \"ISO-8859-1\")\n",
    "a2 = pd.read_csv('data/all-the-news/articles1.csv', encoding = \"ISO-8859-1\")\n",
    "a3 = pd.read_csv('data/all-the-news/articles2.csv', encoding = \"ISO-8859-1\")\n",
    "a4 = pd.read_csv('data/all-the-news/articles3.csv', encoding = \"ISO-8859-1\")\n",
    "raw_documents = [a1, a2, a3, a4]\n",
    "\n",
    "# Format\n",
    "docs = format_raw_documents(raw_documents)\n",
    "\n",
    "# Clear memory\n",
    "del a1, a2, a3, a4, raw_documents\n",
    "\n",
    "docs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "> 1. Merge `Title` & `Content`\n",
    "> 2. Prep text. I set-up to run in parallel and it's much faster but still takes ~7 minutes on my laptop.\n",
    "> 3. Perform Sentiment Analysis using `TextBlob`. May take ~150 hours to run in parallel on my machine. A powerful AWS instance with 64 cores will still take ~9 hours to run. The best approach to optimize performance here is to use convert to **C** code using `Cython` and possibly even use distributed computing.\n",
    "\n",
    "**Next: try optimizing code for efficiency using `Dask` and `Cython`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate `myNLP` object\n",
    "myNLP = myNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b43533a01a44069e0d576e382d89a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=142619), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# merge `Topics` and `Content`\n",
    "merged = merge_2_string_lists(docs['Title'], docs['Content'])\n",
    "\n",
    "# clean and prep text\n",
    "parallel_tasks = merged\n",
    "parallel_func = myNLP.prep_docs_lematize\n",
    "docs['Processed Text'] = parallelize(parallel_func, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3061ef1e74764ab98bfa0ca8dd0c3827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=142619), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "tokenizer must be an instance of BaseTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/dastanaitzhanov1/CloudStation/Job Search/2018/HyperGiant/gdp_prediction/utils.py\", line 299, in get_sentiment\n    tb = Blobber(text, analyzer=NaiveBayesAnalyzer())\n  File \"/anaconda3/lib/python3.6/site-packages/textblob/blob.py\", line 765, in __init__\n    parser, classifier)\n  File \"/anaconda3/lib/python3.6/site-packages/textblob/blob.py\", line 319, in _initialize_models\n    base_class_name=\"BaseTokenizer\")\n  File \"/anaconda3/lib/python3.6/site-packages/textblob/blob.py\", line 308, in _validated_param\n    .format(name=name, cls=base_class_name))\nValueError: tokenizer must be an instance of BaseTokenizer\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a349b2868c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparallel_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Processed Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparallel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ec98ea2860da>\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(function, tasks, workers)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tokenizer must be an instance of BaseTokenizer"
     ]
    }
   ],
   "source": [
    "# perform Sentiment Analysis\n",
    "parallel_tasks = docs['Processed Text']\n",
    "parallel_func = get_sentiment\n",
    "docs['Sentiment'] = parallelize(parallel_func, parallel_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tokenizer must be an instance of BaseTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-78330e3ca012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Processed Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiment_p_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CloudStation/Job Search/2018/HyperGiant/gdp_prediction/utils.py\u001b[0m in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlobber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNaiveBayesAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mopinion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopinion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopinion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjectivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier)\u001b[0m\n\u001b[1;32m    763\u001b[0m                 analyzer=None, parser=None, classifier=None):\n\u001b[1;32m    764\u001b[0m         _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,\n\u001b[0;32m--> 765\u001b[0;31m                             parser, classifier)\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m_initialize_models\u001b[0;34m(obj, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier)\u001b[0m\n\u001b[1;32m    317\u001b[0m                                     \u001b[0mbase_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizerI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                                     \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBaseBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                                     base_class_name=\"BaseTokenizer\")\n\u001b[0m\u001b[1;32m    320\u001b[0m     obj.np_extractor = _validated_param(np_extractor, \"np_extractor\",\n\u001b[1;32m    321\u001b[0m                                         \u001b[0mbase_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBaseNPExtractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m_validated_param\u001b[0;34m(obj, name, base_class, default, base_class_name)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         raise ValueError('{name} must be an instance of {cls}'\n\u001b[0;32m--> 308\u001b[0;31m                          .format(name=name, cls=base_class_name))\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tokenizer must be an instance of BaseTokenizer"
     ]
    }
   ],
   "source": [
    "text = docs['Processed Text'][0]\n",
    "sentiment_p_pos, polarity, subjectivity = get_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999992782957725\n",
      "sindh govt decides cut public transport fare 7pc kti rej karachi sindh government decided bring public transport fare 7 per cent due massive reduction petroleum product price federal government geo news reported source said reduction fare applicable public transport rickshaw taxi mean traveling meanwhile karachi transport ittehad kti refused abide government decision kti president irshad bukhari said commuter charged lowest fare karachi compare part country adding 80pc vehicle run compressed natural gas cng bukhari said karachi transporter cut fare decrease cng price made\n"
     ]
    }
   ],
   "source": [
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "tb = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "print(tb(text).sentiment.p_pos)\n",
    "print(tb(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion = tb(text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.048214285714285716"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Top #1 topic (NMF)\n",
      "1 Top #2 topic (NMF)\n",
      "2 Top #3 topic (NMF)\n",
      "3 Top #4 topic (NMF)\n",
      "4 Top #5 topic (NMF)\n"
     ]
    }
   ],
   "source": [
    "def add_top_5_topics(docs, top_5_topics_NMF):\n",
    "    '''\n",
    "    Adds top 5 topics in separate columns to the `docs` dataframe.\n",
    "    '''\n",
    "    for i, n in enumerate(range(1,6)):\n",
    "        col_name = 'Top #{} topic (NMF)'.format(str(n))\n",
    "        docs[col_name] = top_5_topics_NMF[i]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
