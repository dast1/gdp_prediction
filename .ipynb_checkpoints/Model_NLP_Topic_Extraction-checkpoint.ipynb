{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import own scripts\n",
    "import sys\n",
    "sys.path.insert(0, '/src/')\n",
    "\n",
    "%autoreload 2\n",
    "from utils import format_raw_documents, myNLP, merge_2_string_lists, add_top_5_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: \n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores: \")\n",
    "!sysctl -n hw.ncpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook functions\n",
    "def parallelize(function, tasks, workers=4):\n",
    "    '''\n",
    "    Performs a task as defined by `function` in parallel and returns the result.\n",
    "    '''\n",
    "    if __name__ == '__main__':\n",
    "        with Pool(processes=workers) as p:\n",
    "            results = list(tqdm_notebook(p.imap(function, tasks), total=len(tasks)))   \n",
    "        return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142616</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>oil prices up in asian trad</td>\n",
       "      <td>Singapore: Oil prices edged higher in quiet As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142617</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>cost of quaid e azam solar power project cut b...</td>\n",
       "      <td>LAHORE: Putting  stout defence of the solar po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142618</th>\n",
       "      <td>9/9/2015</td>\n",
       "      <td>china pakistan set to sign cpec zone d</td>\n",
       "      <td>KARACHI: Pakistan is set to sign a 40-year-lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142619</th>\n",
       "      <td>9/9/2016</td>\n",
       "      <td>Global airport traffic 64 percent 2015 fastest...</td>\n",
       "      <td>strong&gt;MONTREAL: Global airport traffic grew a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142620</th>\n",
       "      <td>9/9/2016</td>\n",
       "      <td>Taxpayers directory to be launched today</td>\n",
       "      <td>strong&gt;ISLAMABAD: Finance Minister Ishaq Dar w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                              Title  \\\n",
       "142616  9/9/2015                        oil prices up in asian trad   \n",
       "142617  9/9/2015  cost of quaid e azam solar power project cut b...   \n",
       "142618  9/9/2015             china pakistan set to sign cpec zone d   \n",
       "142619  9/9/2016  Global airport traffic 64 percent 2015 fastest...   \n",
       "142620  9/9/2016           Taxpayers directory to be launched today   \n",
       "\n",
       "                                                  Content  \n",
       "142616  Singapore: Oil prices edged higher in quiet As...  \n",
       "142617  LAHORE: Putting  stout defence of the solar po...  \n",
       "142618  KARACHI: Pakistan is set to sign a 40-year-lea...  \n",
       "142619  strong>MONTREAL: Global airport traffic grew a...  \n",
       "142620  strong>ISLAMABAD: Finance Minister Ishaq Dar w...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load news articles\n",
    "a1 = pd.read_csv('data/Articles.csv', encoding = \"ISO-8859-1\")\n",
    "a2 = pd.read_csv('data/all-the-news/articles1.csv', encoding = \"ISO-8859-1\")\n",
    "a3 = pd.read_csv('data/all-the-news/articles2.csv', encoding = \"ISO-8859-1\")\n",
    "a4 = pd.read_csv('data/all-the-news/articles3.csv', encoding = \"ISO-8859-1\")\n",
    "raw_documents = [a1, a2, a3, a4]\n",
    "\n",
    "# Format\n",
    "docs = format_raw_documents(raw_documents)\n",
    "\n",
    "# Clear memory\n",
    "del a1, a2, a3, a4, raw_documents\n",
    "\n",
    "docs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Extraction\n",
    "> 1. Merge `Title` & `Content`\n",
    "> 2. Process text. I set-up to run in parallel and it's much faster but still takes ~30 minutes on my laptop.\n",
    "> 3. Run `NMF`. Running `NMF` in parallel took ~1.5 hours.\n",
    "> 4. Ready to Run `LDA`. Tried running `LDA` but it take will a long time, more than 5 hrs. \n",
    "\n",
    "**Next: try optimizing code for efficiency using `Dask` and `Cython`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate `myNLP` object\n",
    "myNLP = myNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9fdd91ebb8448ebf00f604bcfa584b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=142619), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# merge `Topics` and `Content`\n",
    "merged = merge_2_string_lists(docs['Title'], docs['Content'])\n",
    "\n",
    "# clean and prep text\n",
    "prep_func = myNLP.prep_docs_stem\n",
    "docs['Processed Text'] = parallelize(prep_func, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run NMF\n",
    "top_5_topics_NMF, top_words_in_topic_NMF, nmf, tfidf, tfidf_vect = myNLP.fit_nmf(docs['Processed Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-eae6ed144975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run NMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop_5_topics_NMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_words_in_topic_NMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Processed Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# add `Top 5 Topics (NMF)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Top 5 Topics (NMF)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_5_topics_NMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CloudStation/Job Search/2018/HyperGiant/HG/utils.py\u001b[0m in \u001b[0;36mfit_nmf\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    182\u001b[0m                        \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                        \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                        init='nndsvd').fit(self.tfidf)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# get top 5 topics for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                                                random_state=random_state)\n\u001b[0m\u001b[1;32m   1024\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         W, H, n_iter = _fit_multiplicative_update(X, W, H, beta_loss, max_iter,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_fit_coordinate_descent\u001b[0;34m(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Update W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         violation += _update_coordinate_descent(X, W, Ht, l1_reg_W,\n\u001b[0;32m--> 485\u001b[0;31m                                                 l2_reg_W, shuffle, rng)\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;31m# Update H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_update_coordinate_descent\u001b[0;34m(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# The following seems to be required on 64-bit Windows w/ Python 3.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_update_cdnmf_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHHt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXHt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# add `Top 5 Topics (NMF)`\n",
    "docs = add_top_5_topics(docs, top_5_topics_NMF)\n",
    "\n",
    "# save\n",
    "top_words_in_topic_NMF.to_csv('data/top_words_in_topic_NMF.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# run LDA \n",
    "top_5_topics_LDA, top_words_in_topic_LDA, lda, tf, cnt_vect = myNLP.fit_lda(docs['Processed Text'])\n",
    "\n",
    "# add `Top 5 Topics (NMF)`\n",
    "docs['Top 5 Topics (LDA)'] = top_5_topics_LDA\n",
    "\n",
    "# save\n",
    "top_words_in_topic_LDA.to_csv('data/top_words_in_topic_LDA.csv', sep=',')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "docs.to_csv('data/nlp.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "> Randomly pull up a few articles in each topic and assess if the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80, 59, 80, ..., 59, 41, 31],\n",
       "       [29, 52, 23, ..., 37, 23, 59],\n",
       "       [46,  9, 84, ..., 73,  3, 11],\n",
       "       [11, 94, 97, ..., 97, 96, 42],\n",
       "       [42, 27, 24, ..., 31, 37, 18]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(top_5_topics_NMF).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
